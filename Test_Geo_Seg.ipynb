{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install numpy==1.23.5 the code works with this version!**"
      ],
      "metadata": {
        "id": "r7smLUQovTqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 --force-reinstall\n"
      ],
      "metadata": {
        "id": "GPDlLoZ1vUIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pip Install Libs and check**"
      ],
      "metadata": {
        "id": "HegUwOHzvfdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install SimpleITK\n",
        "!pip install nibabel\n",
        "\n",
        "# Reinstall packages (if not persisted)\n",
        "!pip install pyvista==0.36.1\n",
        "!pip install trimesh==3.12.6\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-geometric\n",
        "!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "import pyvista\n",
        "import trimesh\n",
        "import pytorch3d\n",
        "\n",
        "print(\" All modules are working after restart.\")\n"
      ],
      "metadata": {
        "id": "f8k8x8-tZ2Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount the Google Drive, My dataset is in drive**"
      ],
      "metadata": {
        "id": "9-GA9_Qqvk_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "WDJaAlzXZfKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the Repo; it contains code, Models etc..,**\n",
        "\n",
        "\n",
        "**Note: you must create the data.json and link it with the dataset.py**\n"
      ],
      "metadata": {
        "id": "BTmk2aldvtoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "%cd /content\n",
        "\n",
        "#  Remove the repo if it exists\n",
        "shutil.rmtree(\"/content/Geo_Seg\", ignore_errors=True)\n",
        "\n",
        "#  Clone repo from scratch\n",
        "!git clone https://github.com/AliakbarMzadeh/Geo_Seg.git\n",
        "\n",
        "#  Now CD into it\n",
        "%cd /content/Geo_Seg\n"
      ],
      "metadata": {
        "id": "C8z0DfNQIgbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This model needs a CSV file, you must genarate it with Meshlab or this code, it used for GCN blocks and actuaalt converts the .stl format to triangle mesh based**"
      ],
      "metadata": {
        "id": "_3er3PSuwLu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Verified STL directory\n",
        "stl_dir = \"/content/drive/MyDrive/Team_Internship_Dataset/Normal/SurfaceMeshes\"\n",
        "\n",
        "# Output path for converted dataset\n",
        "out_dir = \"/content/converted_dataset/CoronaryArtery\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "for i in range(1, 21):\n",
        "    case_name = f\"case{i-1}\"\n",
        "    case_path = os.path.join(out_dir, case_name)\n",
        "    os.makedirs(case_path, exist_ok=True)\n",
        "\n",
        "    # Load STL and sample mesh\n",
        "    mesh_path = os.path.join(stl_dir, f\"Normal_{i}.stl\")\n",
        "    mesh = trimesh.load(mesh_path)\n",
        "\n",
        "    points = mesh.sample(3000)\n",
        "    pd.DataFrame(points).to_csv(os.path.join(case_path, \"mesh.csv\"), index=False, header=False, sep=\" \")\n",
        "    print(f\"Saved: {case_name}/mesh.csv\")\n"
      ],
      "metadata": {
        "id": "ZfHtanXMIgdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**push the data from drive to colab**"
      ],
      "metadata": {
        "id": "yCEhKAKGwvYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "image_dir = \"/content/drive/MyDrive/Team_Internship_Dataset/Normal/CTCA\"\n",
        "label_dir = \"/content/drive/MyDrive/Team_Internship_Dataset/Normal/Annotations\"\n",
        "out_dir = \"/content/converted_dataset/CoronaryArtery\"\n",
        "\n",
        "for i in range(1, 21):\n",
        "    case_dir = os.path.join(out_dir, f\"case{i-1}\")\n",
        "    os.makedirs(case_dir, exist_ok=True)\n",
        "\n",
        "    # Copy as .nrrd (do NOT rename to .nii.gz)\n",
        "    shutil.copy(os.path.join(image_dir, f\"Normal_{i}.nrrd\"), os.path.join(case_dir, \"image.nrrd\"))\n",
        "    shutil.copy(os.path.join(label_dir, f\"Normal_{i}.nrrd\"), os.path.join(case_dir, \"label.nrrd\"))\n",
        "\n",
        "    print(f\"Copied Normal_{i} as .nrrd to {case_dir}\")\n"
      ],
      "metadata": {
        "id": "f-CIvuA5IgiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: in this dataset the number of Z depth is not same for samples, so we must crop them because our batch size is more than 1**"
      ],
      "metadata": {
        "id": "o4R_Yt8Xw8my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import SimpleITK as sitk\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "\n",
        "original_dir = \"/content/converted_dataset/CoronaryArtery\"\n",
        "corrected_dir = \"/content/converted_dataset/CoronaryArtery_fixed\"\n",
        "target_depth = 160\n",
        "\n",
        "os.makedirs(corrected_dir, exist_ok=True)\n",
        "inconsistent_cases = []\n",
        "\n",
        "def crop_or_pad(volume, target_depth):\n",
        "    d, h, w = volume.shape\n",
        "    if d > target_depth:\n",
        "        start = (d - target_depth) // 2\n",
        "        return volume[start:start+target_depth]\n",
        "    elif d < target_depth:\n",
        "        pad_before = (target_depth - d) // 2\n",
        "        pad_after = target_depth - d - pad_before\n",
        "        return np.pad(volume, ((pad_before, pad_after), (0, 0), (0, 0)), mode='constant')\n",
        "    else:\n",
        "        return volume\n",
        "\n",
        "for i in range(20):\n",
        "    case = f\"case{i}\"\n",
        "    case_path = os.path.join(original_dir, case)\n",
        "    img_path = os.path.join(case_path, \"image.nrrd\")\n",
        "    lbl_path = os.path.join(case_path, \"label.nrrd\")\n",
        "\n",
        "    img = sitk.ReadImage(img_path)\n",
        "    lbl = sitk.ReadImage(lbl_path)\n",
        "\n",
        "    img_np = sitk.GetArrayFromImage(img)\n",
        "    lbl_np = sitk.GetArrayFromImage(lbl)\n",
        "\n",
        "    if img_np.shape[0] != target_depth or lbl_np.shape[0] != target_depth:\n",
        "        inconsistent_cases.append(case)\n",
        "\n",
        "        # Crop/pad and save\n",
        "        img_fixed = crop_or_pad(img_np, target_depth)\n",
        "        lbl_fixed = crop_or_pad(lbl_np, target_depth)\n",
        "\n",
        "        # Save\n",
        "        case_fixed_path = os.path.join(corrected_dir, case)\n",
        "        os.makedirs(case_fixed_path, exist_ok=True)\n",
        "        sitk.WriteImage(sitk.GetImageFromArray(img_fixed), os.path.join(case_fixed_path, \"image.nrrd\"))\n",
        "        sitk.WriteImage(sitk.GetImageFromArray(lbl_fixed), os.path.join(case_fixed_path, \"label.nrrd\"))\n",
        "\n",
        "        # Also copy mesh\n",
        "        mesh_src = os.path.join(case_path, \"mesh.csv\")\n",
        "        mesh_dst = os.path.join(case_fixed_path, \"mesh.csv\")\n",
        "        shutil.copy(mesh_src, mesh_dst)\n",
        "\n",
        "if len(inconsistent_cases) == 0:\n",
        "    print(\"All images and labels have consistent Z-depth.\")\n",
        "    final_data_path = original_dir\n",
        "else:\n",
        "    print(\"Found inconsistent Z-depths in cases:\", inconsistent_cases)\n",
        "    print(f\" Saved corrected files to: {corrected_dir}\")\n",
        "    final_data_path = corrected_dir\n",
        "\n",
        "# Rewrite data.json to match the final path\n",
        "print(\" Writing new data.json...\")\n",
        "data_json = {}\n",
        "base_path = os.path.basename(final_data_path)\n",
        "\n",
        "for i in range(20):\n",
        "    case = f\"case{i}\"\n",
        "    data_json[case] = [{\n",
        "        \"image\": f\"{base_path}/{case}/image.nrrd\",\n",
        "        \"label\": f\"{base_path}/{case}/label.nrrd\",\n",
        "        \"verts\": f\"{base_path}/{case}/mesh.csv\"\n",
        "    }]\n",
        "\n",
        "with open(\"/content/converted_dataset/data.json\", \"w\") as f:\n",
        "    json.dump(data_json, f, indent=2)\n",
        "\n",
        "print(\"Final data.json updated.\")\n"
      ],
      "metadata": {
        "id": "RbCCMhJBIgk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduce the Batch size, num_worker, total_epoches because the data is 3D medical image and the U-net model is too dense, it needs heavy RAM and GPU**\n",
        "\n",
        "**The paper mentioned we need: NVIDIA A100 (80GB) GPU**"
      ],
      "metadata": {
        "id": "BCbXs1z9xMSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_path = \"/content/Geo_Seg/config/config-s1-train.yaml\"\n",
        "\n",
        "with open(yaml_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Patch dataset loading\n",
        "config['dataset']['batch_size'] = 1          # smaller batch to reduce memory\n",
        "config['dataset']['num_worker'] = 0          # prevent worker overload\n",
        "\n",
        "#  Patch training loop\n",
        "config['trainer']['total_epoches'] = 2      # for test runs\n",
        "config['trainer']['current_epoch'] = 1\n",
        "\n",
        "\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(\" Patched config-s1-train.yaml: batch_size=1, num_worker=0, epochs=1\")"
      ],
      "metadata": {
        "id": "H93AFk7GIgpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the Stage 1**"
      ],
      "metadata": {
        "id": "PcT5iK0axt_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Geo_Seg\n",
        "!python3 train.py -c ./config/config-s1-train.yaml | tee train_stage1_output.log\n"
      ],
      "metadata": {
        "id": "UWIb0PQgIgsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plots the info after Stage 1**"
      ],
      "metadata": {
        "id": "9x3PhGJtxyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the log file\n",
        "log_path = \"/content/Geo_Seg/train_stage1_output.log\"\n",
        "with open(log_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Initialize metric lists\n",
        "train_epochs, train_dice, train_chamfer, train_loss = [], [], [], []\n",
        "eval_dict = {}\n",
        "\n",
        "for line in lines:\n",
        "    if \"[TRAIN][Epoch\" in line:\n",
        "        match = re.search(r\"\\[TRAIN\\]\\[Epoch (\\d+)\\] Dice: ([0-9.]+) \\| Chamfer: ([0-9.eE+-]+)\", line)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            train_epochs.append(epoch)\n",
        "            train_dice.append(float(match.group(2)))\n",
        "            train_chamfer.append(float(match.group(3)))\n",
        "    elif \"[EVAL][Epoch\" in line:\n",
        "        match = re.search(r\"\\[EVAL\\]\\[Epoch (\\d+)\\] Dice: ([0-9.]+) \\| Chamfer: ([0-9.eE+-]+)\", line)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            eval_dict[epoch] = (float(match.group(2)), float(match.group(3)))\n",
        "    elif \"finished ! Loss:\" in line:\n",
        "        match = re.search(r\"Epoch(\\d+).*Loss: ([0-9.]+)\", line)\n",
        "        if match:\n",
        "            train_loss.append(float(match.group(2)))\n",
        "\n",
        "# Align eval metrics with train epochs\n",
        "eval_epochs, eval_dice, eval_chamfer = [], [], []\n",
        "for ep in train_epochs:\n",
        "    if ep in eval_dict:\n",
        "        eval_epochs.append(ep)\n",
        "        eval_dice.append(eval_dict[ep][0])\n",
        "        eval_chamfer.append(eval_dict[ep][1])\n",
        "\n",
        "# === Plots === #\n",
        "\n",
        "# Training Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_epochs, train_loss, marker='o', color='blue')\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Training Dice\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_epochs, train_dice, marker='o', color='blue')\n",
        "plt.title(\"Training Dice per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Dice\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluation Dice\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(eval_epochs, eval_dice, marker='o', color='orange')\n",
        "plt.title(\"Evaluation Dice per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Dice\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Training Chamfer\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_epochs, train_chamfer, marker='o', color='blue')\n",
        "plt.title(\"Training Chamfer Distance per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Chamfer Distance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluation Chamfer\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(eval_epochs, eval_chamfer, marker='o', color='orange')\n",
        "plt.title(\"Evaluation Chamfer Distance per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Chamfer Distance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i6gncElhIgud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep \"Epoch\" train_stage1_output.log\n",
        "!grep \"dice\" train_stage1_output.log\n",
        "!grep \"chamfer distance\" train_stage1_output.log"
      ],
      "metadata": {
        "id": "onrJGsuzi8sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXWrbd1Ji8xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 2**"
      ],
      "metadata": {
        "id": "9au_2OB8x9nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduce the Batch, num_worker, total_epoches like stage 1**"
      ],
      "metadata": {
        "id": "3lMgY8Qkx_7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yaml\n",
        "\n",
        "yaml_path = \"/content/Geo_Seg/config/config-s2-train.yaml\"\n",
        "\n",
        "with open(yaml_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Patch settings for Colab and single-sample validation\n",
        "config['dataset']['batch_size'] = 1\n",
        "config['dataset']['num_worker'] = 0\n",
        "config['trainer']['total_epoches'] = 2\n",
        "config['trainer']['current_epoch'] = 1\n",
        "\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(\" Patched config-s2-train.yaml for minimal test.\")\n"
      ],
      "metadata": {
        "id": "U9i72YEEi87x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Stage 2**"
      ],
      "metadata": {
        "id": "esDmGUq1yLhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Geometry_Segmentation_for_Coronary_Artery\n",
        "!python3 train.py -c ./config/config-s2-train.yaml | tee train_stage2_output.log\n"
      ],
      "metadata": {
        "id": "owOGXZhwIgxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the info**"
      ],
      "metadata": {
        "id": "YNW-oyA2yPHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the log file for stage 2\n",
        "log_path = \"/content/Geometry_Segmentation_for_Coronary_Artery/train_stage2_output.log\"\n",
        "with open(log_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Initialize metric lists\n",
        "train_epochs, train_dice, train_chamfer, train_loss = [], [], [], []\n",
        "eval_dict = {}\n",
        "\n",
        "# Parse training and evaluation metrics\n",
        "for line in lines:\n",
        "    if \"[TRAIN][Epoch\" in line:\n",
        "        match = re.search(r\"\\[TRAIN\\]\\[Epoch (\\d+)\\] Dice: ([0-9.]+) \\| Chamfer: ([0-9.eE+-]+)\", line)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            train_epochs.append(epoch)\n",
        "            train_dice.append(float(match.group(2)))\n",
        "            train_chamfer.append(float(match.group(3)))\n",
        "    elif \"[EVAL][Epoch\" in line:\n",
        "        match = re.search(r\"\\[EVAL\\]\\[Epoch (\\d+)\\] Dice: ([0-9.]+) \\| Chamfer: ([0-9.eE+-]+)\", line)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            eval_dict[epoch] = (float(match.group(2)), float(match.group(3)))\n",
        "    elif \"finished ! Loss:\" in line:\n",
        "        match = re.search(r\"Epoch(\\d+).*Loss: ([0-9.]+)\", line)\n",
        "        if match:\n",
        "            train_loss.append(float(match.group(2)))\n",
        "\n",
        "# Align eval metrics with train epochs\n",
        "eval_epochs, eval_dice, eval_chamfer = [], [], []\n",
        "for ep in train_epochs:\n",
        "    if ep in eval_dict:\n",
        "        eval_epochs.append(ep)\n",
        "        eval_dice.append(eval_dict[ep][0])\n",
        "        eval_chamfer.append(eval_dict[ep][1])\n",
        "\n",
        "# Define plotting function\n",
        "def plot_metric(x, y, title, ylabel, color):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(x, y, marker='o', color=color)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot each metric separately\n",
        "plot_metric(train_epochs, train_loss, \"Stage 2 - Training Loss per Epoch\", \"Loss\", \"blue\")\n",
        "plot_metric(train_epochs, train_dice, \"Stage 2 - Training Dice per Epoch\", \"Dice\", \"blue\")\n",
        "plot_metric(eval_epochs, eval_dice, \"Stage 2 - Evaluation Dice per Epoch\", \"Dice\", \"orange\")\n",
        "plot_metric(train_epochs, train_chamfer, \"Stage 2 - Training Chamfer Distance per Epoch\", \"Chamfer Distance\", \"blue\")\n",
        "plot_metric(eval_epochs, eval_chamfer, \"Stage 2 - Evaluation Chamfer Distance per Epoch\", \"Chamfer Distance\", \"orange\")\n"
      ],
      "metadata": {
        "id": "sqisJbZ3Ig0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**"
      ],
      "metadata": {
        "id": "4PvYthKryWXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pass the weight to the prediction part and reduce the parameters**"
      ],
      "metadata": {
        "id": "LBPELTuJyZWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "cfg_path = \"/content/Geometry_Segmentation_for_Coronary_Artery/config/config-predict.yaml\"\n",
        "ckpt_path = \"./checkpoints/Tag-GeometrySegmentation-CoronaryArtery-s2-latest-checkpoint.pth\"  # ✅ Safe file\n",
        "\n",
        "with open(cfg_path, \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "#  Patch checkpoint paths\n",
        "cfg['network']['modules']['Unet']['cur_params'] = ckpt_path\n",
        "cfg['network']['modules']['Gseg']['cur_params'] = ckpt_path\n",
        "\n",
        "#  Reduce batch size and number of workers for CPU\n",
        "cfg['dataset']['batch_size'] = 2\n",
        "cfg['dataset']['num_worker'] = 0\n",
        "cfg['dataset']['is_shuffle'] = False  #  makes prediction reproducible\n",
        "\n",
        "with open(cfg_path, \"w\") as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "print(f\" Prediction config updated to use: {ckpt_path} with CPU-safe settings.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1D28NzStIg20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Prediction**"
      ],
      "metadata": {
        "id": "fAh9RxSDygMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Geo_Seg\n",
        "!python3 predict.py -c ./config/config-predict.yaml"
      ],
      "metadata": {
        "id": "B1G_bBztIg5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Geo_Seg/predict/right-predict-0.stl\")\n",
        "files.download(\"/content/Geo_Seg/CoronaryArtery-pointcloud.xyz\")"
      ],
      "metadata": {
        "id": "b_LTnaG9Ig8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}